import csv
import time
import argparse
import os
import pandas as pd
from datetime import datetime
from pylsl import StreamInlet, resolve_streams
from typing import Tuple


def find_stream(
    stream_names: list[str],
) -> Tuple[list[StreamInlet], dict]:
    """
    Finds the LSL stream by name and its info, then returns a StreamInlet
    for data collection.

    Args:
        stream_names (list[str]): The names of the stream to find.

    Returns:
        inlets (list[StreamInlet]): The inlets for the found stream.
        stream_channel_labels (dict): A dictionary of labels from each inlets
    """
    print("Looking for EEG streams")
    # Find streams and get their metadata
    inlets = []
    # Dictionary to hold channel labels for each stream
    stream_channel_labels = {}

    all_streams = resolve_streams(wait_time=5.0)

    for name in stream_names:
        stream_found = False
        for stream in all_streams:
            if stream.name() == name:
                print(f"Found stream: {name}")
                inlet = StreamInlet(stream)
                inlets.append(inlet)

                info = inlet.info()
                ch = info.desc().child("channels").child("channel")
                labels = []
                for _ in range(info.channel_count()):
                    labels.append(ch.child_value("label"))
                    ch = ch.next_sibling()
                stream_channel_labels[name] = labels
                print(f"  > Found channels: {labels}")

                stream_found = True
                break
        if not stream_found:
            print(f"Warning: Stream '{name}' not found.")

    if not inlets:
        raise RuntimeError("Error: No specified LSL streams were found. Cannot continue.")
    return inlets, stream_channel_labels


def unified_receive(inlets, duration: int) -> str:
    """
    Records multiple LSL streams, fetches their channel names, and formats
    the output into a single "wide" format CSV with descriptive headers.
    """
    print("--- Unified Recorder ---")

    # Record data to a temporary "long" format file
    timestamp = datetime.now().strftime("%Y%m%d-%H%M%S")
    temp_filename = f"temp-{timestamp}.csv"
    max_channels = max(inlet.info().channel_count() for inlet in inlets)
    csv_headers = ["lsl_timestamp", "stream_name"] + [f"value_ch{i+1}" for i in range(max_channels)]

    print(f"\nRecording raw data to temporary file: {temp_filename}")
    with open(temp_filename, "w", newline="") as f:
        writer = csv.writer(f)
        writer.writerow(csv_headers)
        start_time = time.time()
        while time.time() - start_time < duration:
            for inlet in inlets:
                sample, timestamp = inlet.pull_sample(timeout=0.0)
                if sample:
                    row = [timestamp, inlet.info().name()] + sample
                    writer.writerow(row)
    print("--- Raw recording finished. ---")

    return temp_filename


def format_csv(final_filename_base: str, temp_filename: str, stream_channel_labels: dict):
    # Format the data using the fetched channel names
    try:
        df = pd.read_csv(temp_filename)

        streams = {}
        for name in df["stream_name"].unique():
            stream_df = df[df["stream_name"] == name].copy()
            stream_df.drop("stream_name", axis=1, inplace=True)
            stream_df.dropna(axis=1, how="all", inplace=True)

            # Rename generic columns (value_ch1) to real channel names
            original_cols = [col for col in stream_df.columns if col.startswith("value_ch")]
            real_labels = stream_channel_labels.get(name, [])
            # Create unique final names like "StreamName_ChannelName"
            rename_dict = {orig: f"{name}_{label}" for orig, label in zip(original_cols, real_labels)}
            stream_df.rename(columns=rename_dict, inplace=True)

            streams[name] = stream_df

        headset_stream_name = max(streams, key=lambda name: streams[name].shape[1])
        marker_stream_name = min(streams, key=lambda name: streams[name].shape[1])

        headset_df = streams[headset_stream_name]
        marker_df = streams[marker_stream_name]

        # Sort for merge_asof
        headset_df = headset_df.sort_values("lsl_timestamp")
        marker_df = marker_df.sort_values("lsl_timestamp")

        formatted_df = pd.merge_asof(
            left=headset_df, right=marker_df, on="lsl_timestamp", direction="backward", tolerance=0.5
        )
        # Fill NaN for marker columns and set type to int
        marker_cols = [col for col in formatted_df.columns if marker_stream_name in col]
        for col in marker_cols:
            formatted_df[col] = formatted_df[col].fillna(0).astype(int)

        timestamp_str = datetime.now().strftime("%Y%m%d-%H%M%S")
        final_filename = f"{final_filename_base}-{timestamp_str}.csv"

        print(f"\nSaving formatted data to: {final_filename}")
        formatted_df.to_csv(final_filename, index=False)

    except Exception as e:
        print(f"An error occurred during formatting: {e}")
    finally:
        print(f"Cleaning up temporary file: {temp_filename}")
        os.remove(temp_filename)


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="LSL Unified Recorder and Formatter")
    parser.add_argument("--streams", nargs="+", required=True, help="List of LSL stream names.")
    parser.add_argument("--duration", type=int, required=True, help="Recording duration in seconds.")
    parser.add_argument("--filename", type=str, required=True, help="Base for the final output filename.")

    args = parser.parse_args()
    inlets, stream_channel_labels = find_stream(args.streams)
    temp_filename = unified_receive(inlets, args.duration)
    format_csv(args.filename, temp_filename, stream_channel_labels)
